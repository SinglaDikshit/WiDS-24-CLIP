Context Op-timization (CoOp)
CoOp effectively turns pre-trained vision-language models into data-efficient visual learners
a simple approach specifically foradapting CLIP-like vision-language models for down-stream image recognition. Concretely, CoOp models aprompt’s context words with learnable vectors while the entire pre-trained parameters are kept fixed. To handle different image recognition tasks, we provide two implementations of CoOp: unified context and class-specific context.
These vision language models are trained on a small discrete labeled dataset and making them unable to deal with new categories since ad-ditional data are required for learning a new classifier.
CLIP and ALIGN -visual representation learing lodels. They formulate the learning objective as a contrastive loss,which pulls together images and their textual descriptions while pushes away unmatched pairs in the feature space. 
Prompt writing is very important prompt engineering also requires prior knowledge about the task and ideally the language model’s underlying mechanism.Tuning the sentence structure could bring further improvements.
Context Optimization (CoOp) to automate prompt engineering, specifically for pre-trained vision-language models. CoOp models a prompt’s context words with learnable vectors, which could be initialized with either random values or pretrained word embeddings.
Two implementation- One is unified context, which shares the same context with all classes and works well on mostcategories; while the other is based on class-specific context, which learns a specific set of context tokens for each class and is found to be more suitable for some fine-grained categories.
we simply minimize prediction errors using the cross-entropy loss with respect to the learnable context vectors while keeping the entire pre-trained parameters fixed. The gradients can be back-propagated all the way through the text encoder, distilling the rich knowledge encoded in the parameters for learning task-relevant context.
CoOp demonstrate stronger robustness than the zero-shot model (which uses manual prompts) to domain shifts, despite being a learning-based approach. outperforms both handcrafted prompts and the linear probe model in terms of downstream transfer learning performance and robustness under domain shifts for large ViLanguage models.
Related Work
Vision language models are driven i)text representation learning with transformer ii)large mini-batch representation learning iii) web-scale traning datasets. Matching images and text features has been formulated as metric learning , multilabel classification, n-gram language learning, and the recently proposed captioning.
Prompt Learning in NLP: The basic idea of knowledge probing is to induce pre-trained language models to generate answers given cloze-style prompts, which can benefit a number of downstream tasks, such as sentiment analysis. Continuous prompt learning methods which optimize continuous vectors in the word-embedding space. A drawback of such methods compared to searching discrete tokens is the lack of a clear way to visualize what “words” are learned for the vectors.
Models CLIP consists of two encoders, one for images and the other for text. The image encoder aims to map high-dimensional images into low-dimensional
embedding space. The architecture of the image encoder can take the form of a CNN like ResNet-50 or a ViT. On the other hand, the text encoder is built on top of a Transformer and aims to generate text representations from natural language.
CLIP first converts tokens(words) into lower cased byte pair encoding representation. To facilitate mini-batch processing each text sequence is encompassed with EOS and SOS tokens capped at fixed length(77). IDs are mapped to 512-D word embedding vectors, which are then passed on to the Transformer. Finally, the features at the [EOS] token position are layer normalized and further processed by a linear projection layer.
CLIP is trained to align the two embedding spaces learned for images and text. learning objective is formulated as a contrastive loss
CLIP is pre-trained to predict whether an image matches a textual description, it naturally fits zero-shot recognition. This is achieved by comparing image features with the classification weights synthesized by the text encoder, which takes as input textual descriptions specifying classes of interest

